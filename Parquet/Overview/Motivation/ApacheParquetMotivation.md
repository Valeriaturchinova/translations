## Мотивация

Apache Parquet был разработан, чтобы удовлетворить потребность разработчиков из экосистемы Hadoop в колоночном формате данных с возможностью их сжатия.

Parquet создан с учетом необходимости поддержки сложных структур данных. 
Он использует [алгоритм разбиения и восстановления данных](https://github.com/julienledem/redelm/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper) из 
[статьи](https://research.google/pubs/pub36632/) про систему анализа данных Dremel. Ожидается, что этот алгоритм превосходит подход с обычной распаковкой вложенных структур данных.

Parquet создан для обеспечения поддержки высокоэффективных схем 
сжатия и их кодирования. Многочисленные проекты показывают лучшую производительность
при использовании правильной схемы сжатия и кодирования к данным. 
Parquet позволяет указывать схемы сжатия на уровне каждого столбца, а также готов 
к добавлению новых схем кодирования по мере их изобретения и реализации.

Любой разработчик может использовать Parquet. 
В экосистеме Hadoop имеется большое количество фреймворков для обработки данных, 
и каждый их них важен по-своему. Эффективная колоночная структура хранения данных 
без (сложных) зависимостей может быть полезна для всех фреймворков. 
